# **Section 1: Assignment Overview (作业总览)**

* **核心目标**: 这次作业的最终目的是让你**从零开始，亲手编写、组装并训练一个完整的Transformer语言模型**。

* **“从零开始”的严格规则**: 这是作业最硬核的要求。你**不可以使用** PyTorch 中那些现成的高级模块，比如 `torch.nn.Linear` (线性层)、`torch.nn.functional` 里的激活函数、或者 `torch.optim.AdamW` (优化器)。你必须自己动手实现它们。课程**只允许**你使用一些基础工具：
    * `torch.nn.Parameter`: 用来声明一个张量是需要被学习的模型参数。
    * `torch.nn.Module`, `ModuleList` 等容器：这些是用来组织和管理你代码结构的“架子”，本身不包含算法逻辑。
    * `torch.optim.Optimizer`：这是一个“基类”，你需要继承它来实现你自己的优化器，而不是直接使用现成的。

* **代码文件结构**:
    * `cs336_basics/` 文件夹：这是你的**主战场**，你所有的核心代码（比如模型、分词器）都要写在这里面。
    
    * `adapters.py` 文件：这是一个**“转接头”或“适配器”**。它的唯一作用是把你在 `cs336_basics/` 里写的函数，连接到官方的测试程序上。这个文件里不应该有任何复杂的算法逻辑，只是简单的函数调用。

      `adapters.py` 文件已经提供好了，你不需要从零写。你的任务是把它里面的每一个函数都“填”上你自己的实现，让它成为连接你和测试系统的桥梁。
    
    * `test_*.py` 文件：这些是**官方的考卷**，包含了所有你必须通过的测试。你不能修改这些文件，你的任务是让你自己的代码能够通过这些测试的检验。

---

# **Section 2: BPE Tokenizer (BPE分词器)**

## **Key English Content**

* **Overall Goal**: "train and implement a byte-level byte-pair encoding (BPE) tokenizer."

***

## **Technical Background**

* Training tokenizers directly on Unicode codepoints is impractical due to the "prohibitively large" and "sparse" vocabulary (around 150K items).
* The solution is to use a Unicode encoding like UTF-8, which converts characters into a sequence of bytes (integers in the range 0 to 255). This provides a "manageable" initial vocabulary of 256 items.
* However, tokenizing text into bytes results in "extremely long input sequences".
* Subword tokenization is a "midpoint" solution. BPE is a compression algorithm that "iteratively replaces ('merges') the most frequent pair of bytes with a single, new unused index."

***

## **Implementation Requirements**

* **Pre-tokenization**: You should first use a provided regex pattern to split the text into "coarse-grained" tokens. This is done for efficiency and better handling of punctuation. Merges should not happen across these pre-token boundaries.
* **Special Tokens**: Your implementation must handle special tokens (e.g., `<|endoftext|>`) which "should never be split into multiple tokens".
* **Written Problems**: The section includes written questions about Unicode and UTF-8 (`Problem (unicode1)` and `Problem (unicode2)`).
* **Programming Problems**: You must implement `Problem (train_bpe)` (a function to train the tokenizer) and `Problem (tokenizer)` (a class to perform encoding and decoding).

## **中文翻译与讲解**

* **核心目标**: 训练并实现一个**基于字节的BPE分词器**。

* **技术背景梳理**:
    * **为什么不用Unicode字符直接分词？** 因为世界上有大约15万个Unicode字符，词汇表会变得过于庞大和稀疏，模型难以学习。
    * **第一步：转成字节。** 解决方案是，我们先把所有文本通过 `UTF-8` 编码规则，统一转换成由0-255的整数（即字节）组成的序列。这样我们的初始“原子词汇表”就只有256个，非常小且可控。
    * **但纯字节分词有什么问题？** 它会导致序列变得**极度长**（比如一个汉字变成3个数字）。对于计算量和序列长度的平方相关的Transformer来说，这是灾难性的。
    * **最终方案：BPE子词分词。** BPE算法就是用来解决这个问题的折中方案。它的核心思想是一个压缩算法：**不断地在字节序列中找出出现频率最高的一对相邻字节，然后用一个全新的、未使用过的新ID来替换它们（这个过程就叫“合并”）**。

* **具体实现要求**:
    * **预分词 (Pre-tokenization)**：在正式开始合并字节对之前，你需要先用课程提供的一个正则表达式（regex），把大段文本粗略地切分成一些初始片段。这么做是为了提高效率，并且能更好地处理单词和标点。后续的合并操作只在这些小片段内部进行，**不能跨越片段的边界**。
    * **特殊 Token**: 你的代码必须能处理一些“特殊字符串”（比如代表文章结尾的 `<|endoftext|>`）。这些特殊字符串在任何时候都**不能被切分开**，必须被当作一个整体。
    * **书面问题**: 这一章包含了一些关于Unicode和UTF-8的理论问答题，你需要回答它们。
    * **编程任务**: 你需要完成两个核心编程任务：一个是`train_bpe`函数，用来在文本上训练分词器并学习到合并规则；另一个是`Tokenizer`类，用来加载这些规则并对新文本进行编码和解码。

好的，我们已经清晰地理解了BPE分词器的具体实现要求。

根据我们制定的学习计划，您现在的任务是正式开始动手完成作业。第一个任务就是完成PDF中§2部分的**书面问答题**。这是一个很好的起点，因为它能确保您在写代码前，已经彻底巩固了理论基础。

# 任务需求

## **Key English Content**

* **Deliverable**: "Write a function that, given a path to an input text file, trains a (byte-level) BPE tokenizer."
* **Input Parameters**: "Your BPE training function should handle (at least) the following input parameters:"
    * `input_path: str`: "Path to a text file with BPE tokenizer training data."
    * `vocab_size: int`: "A positive integer that defines the maximum final vocabulary size (including the initial byte vocabulary, vocabulary items produced from merging, and any special tokens)."
    * `special_tokens: list[str]`: "A list of strings to add to the vocabulary."
* **Return Values**: "Your BPE training function should return the resulting vocabulary and merges:"
    * `vocab: dict[int, bytes]`: "The tokenizer vocabulary, a mapping from int (token ID in the vocabulary) to bytes (token bytes)."
    * `merges: list[tuple[bytes, bytes]]`: "A list of BPE merges produced from training. Each list item is a tuple of bytes (\<token1\>, \<token2\>), representing that \<token1\> was merged with \<token2\>. The merges should be ordered by order of creation."

-----

## **通俗易懂的中文翻译与讲解**

* **核心任务 (Deliverable)**：你需要编写一个Python函数，这个函数的功能是：读取一个文本文件，并在这个文件上训练一个基于字节的BPE分词器。

* **函数需要接收的输入参数 (Input Parameters)**：你的函数必须能接收以下三个参数：
    1.  `input_path`: 这是一个字符串，代表了用于训练的文本文件的**文件路径**（比如 `"/data/tinystories.txt"`）。
    2.  `vocab_size`: 这是一个整数，代表你希望最终生成的**词汇表的最大尺寸**。例如，如果设为10000，那么你的函数在初始的256个字节基础上，最多再进行 `10000 - 256 = 9744` 次合并操作。这个数字包括了初始字节、合并产生的新词以及特殊token。
    3.  `special_tokens`: 这是一个由字符串组成的列表，包含了所有需要被当作**特殊整体**处理的token（例如 `['<|endoftext|>']`）。
    
* **函数需要返回的输出结果 (Return Values)**：你的函数在执行完毕后，必须返回两样东西：

    1.  `vocab` (词汇表): 这是一个**字典**。
        * 它的**键**是整数（代表 token ID）。
        * 它的**值**是 `bytes` 对象（代表该 ID 对应的具体字节内容）。
        * 例如，`{97: b'a', 256: b'ba', ...}`。
    2.  `merges` (合并规则): 这是一个**列表**，里面记录了所有发生过的合并操作。
        * 列表中的每一个元素都是一个**元组**，元组里包含两个 `bytes` 对象，例如 `(b'b', b'a')`。
        * 这个列表必须是**有序的**，严格按照合并发生的先后顺序排列。这一点非常重要，因为后续的编码过程需要按照这个顺序来应用规则。

# 任务安排

1.  **创建你的工作文件**
    首先，我们需要一个地方来写代码。

      * **任务**: 在 `cs336_basics/` 文件夹中，创建一个新的Python文件，命名为 `tokenizer.py`。
      * **目的**: 这个文件将专门用来存放你所有与分词器相关的代码。

2.  **定义函数框架**
    在 `tokenizer.py` 文件中，根据PDF的要求，写下你的函数框架：

    ```python
    def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]):
        # 这里将是你的主战场
        
        # 最终你需要返回一个词汇表 (vocab) 和一个合并规则列表 (merges)
        # vocab 的类型: dict[int, bytes]
        # merges 的类型: list[tuple[bytes, bytes]]
        
        pass # 这是一个占位符，你可以删掉它开始写代码
    ```

3.  **分解实现步骤 (从简单到复杂)**
    现在，我们把这个复杂的任务分解成几个更小的、可管理的步骤。强烈建议你先实现一个基础版，然后再逐步增加复杂功能。

      * **第一阶段：实现核心合并逻辑 (参考 lecture\_01.py)**

        1.  **读取少量数据**: 为了快速调试，先不要读取整个TinyStories数据集。可以从`tests/fixtures/`文件夹里找一个小文件（比如`tinystories_sample.txt`）来做实验，甚至直接用一个字符串 `text = "low lower newest"`。
        2.  **初始化**: 像 `lecture_01.py` 一样，初始化一个包含256个基础字节的 `vocab`。
        3.  **核心循环**: 实现 `for` 循环，在循环中完成：
              * **统计词对频率**: 编写代码统计相邻字节对的频率。
              * **找出最高频**: 找到出现次数最多的那一对。注意：PDF中有一个重要的平局规则 (tie-breaking rule)：“如果频率相同，选择字典序更大的那一对”。你需要实现这个逻辑。
              * **执行合并**: 调用 `merge` 函数（你可以先把 `lecture_01.py` 里的版本复制过来用），更新你的字节序列。
              * **记录成果**: 更新 `vocab` 和 `merges`。

      * **第二阶段：加入预分词 (Pre-tokenization)**

        1.  **引入 Regex**: 根据PDF §2.4 的描述，你需要使用官方提供的正则表达式 `PAT` 来对文本进行预分词。
        2.  **修改逻辑**: 你的BPE合并操作现在只能在每一个预分词块内部进行，不能跨越块的边界。你需要修改你的频率统计和合并逻辑来遵守这个规则。
        3.  **提示**: 你可以先对整个文本进行预分词，得到一个单词列表。然后对列表中的每一个单词，应用你之前写好的核心BPE合并逻辑。

      * **第三阶段：处理特殊 Token**

        1.  **分割文本**: 根据PDF §2.5 的描述，在进行预分词之前，你需要先用特殊 `special_tokens` (比如 `<|endoftext|>`) 来分割整个文本。
        2.  **独立处理**: 对分割出来的每一段文本，独立地执行你的预分词和BPE合并流程。这确保了合并永远不会跨越特殊token。
        3.  **更新词汇表**: 别忘了把这些 `special_tokens` 也加入到最终的 `vocab` 中。

      * **第四阶段：连接测试并通过**

        1.  **填充 adapters.py**: 打开 `tests/adapters.py` 文件，找到 `run_train_bpe` 函数。
        2.  **连接代码**: 在该函数中，导入你在 `cs336_basics/tokenizer.py` 中写好的 `train_bpe` 函数，调用它，并返回 `vocab` 和 `merges`。
        3.  **运行测试**: 在你的终端中（确保已激活`.venv`环境），运行测试命令：
            ```bash
            uv run pytest tests/test_train_bpe.py
            ```
        4.  **调试**: 根据测试的报错信息，回到你的代码中进行修改，直到所有测试都显示\*\*“PASS”\*\*。

**建议**：请从第一阶段开始，先在小数据上把核心逻辑跑通。然后再逐步加入预分词和特殊token处理。每完成一小步，都进行测试，这样可以大大降低调试的难度。

# 任务具体实施

## 创建初始词汇表

- 由utf-8里面的256个和spectial_tokens里面的组成

## 预处理大文件

- 创建spectial_tokens对应的正则表达式
- 然后先用“特殊”正则表达式对从文件里读取的每一行进行切分
- 再用给定的PAT正则表达式进行分割，统计分割出每一个部分和对应数量，记录在字典中返回
  - 特殊字符不用记录在字典中
